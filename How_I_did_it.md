Выделим под Итоговое задание отдельную папку, в которой потом далее объявим репозиторий git:

```bash
$ mkdir Documents/y-y-final
$ cd Documents/y-y-final
$ git init
```

изначально так и работаем - из под локального пользователя (встречал в чате что у людей как раз таки запускавших приложение из под рута - приложение ругалось с сарказмом:-) ).

Скачиваем само приложение:

```bash
$ wget https://storage.yandexcloud.net/final-homework/bingo
```

Пробежимся поглядим что оно вообще из себя представляет:

```bash
$ file bingo
bingo: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=7PHOtgNzLcwX7-Bzugf1/EdQYZcMp80WIrKIvs4C9/cCxp-3U9Pg89ljURta9o/hFV8Si2Caw7pBesBwdFI, with debug_info, not stripped
```

- понятно что это уже готовый скомпилированный исполняемый файл код которого изначально писан на языке Go.

Дадим права на запуск:  
`$ chmod +x ./bingo`

Запустим:

```bash
$ ./bingo
Hello world
```

- не многословно.  
  Поглядим на наличие стандартного во многих проектах ключа -h:

```bash
$ ./bingo -h
bingo

Usage:
   [flags]
   [command]

Available Commands:
  completion           Generate the autocompletion script for the specified shell
  help                 Help about any command
  prepare_db           prepare_db
  print_current_config print_current_config
  print_default_config print_default_config
  run_server           run_server
  version              version

Flags:
  -h, --help   help for this command

Use " [command] --help" for more information about a command.
```

Далее играемся запускаем по каждой из доступных субкомманд, дословно не буду приводить, наигрался в целом уже, опишу итогово какой в итоге смысл. Основная подсказка в субкомманде '**print\_default\_config**', она выведет нам дефолтный конфиг приложения, из которого становится ясно, что БД используется - **postgresql**, перечислены параметры подключения (адрес, порт), и креденшиалсы с именем самой БД. И, собственно, не имея самой структуры БД тут имеется субкоманда '**prepare\_db**', которая то и создаст нам саму БД с таблицами, теперь нам нужна сама СУБД, чтобы вдохнуть начальный этап жизни приложения.

### Установка СУБД PostgreSql.

Моя подопытная машина: виртуальная машина c Ubuntu 22.04, проходим на официальную доку 16-ой версии (ставить будем ее), идем прямо по официальной инструкции (т.е. объявляем в ОС репозиторий, делаем доверенными их gpg-ключи и собственно sudo apt update; sudo apt install postgresql). Т.к. моя основная ОС: Windows 10, и на ней уже установлен pgAdmin4 8.0, то нам далее для удобства нужно добиться, чтобы инстанс БД висел портом на своем "реальном" (не loopback) интерфейсе:

- явно объявляем ключ listen:  
  `$ sudo vi /etc/postgresql/16/main/postgresql.conf`  
  добавляем строку:  
  `listen_addresses = '*'`  
  -, при этом также включаю по привычке (да и по моим сохраненным сниппетам конфигурирования БД) тип шифрования scram-sha-256, добавляем:  
  `password_encryption = scram-sha-256`  
  -, остальное оставляем пока без изменений, сохраняемся, выходим.

Теперь нужно объявить системе аутентификации postgresql что будем подключаться удалённо, добавляем в файл /etc/postgresql/16/main/pg\_hba.conf строку:  
`host all postgres <my_win10_net-ip>/32 scram-sha-256`  
-, вместо &lt;my\_win10\_net-ip&gt; - IP моей основной машины Win10, сохраняемся и перезапускаем сервис postgresql.

Теперь можем объявить пароль дефолтному админ.юзеру 'postgres' (как раз и пароль зашифруется уже сразу в scram-sha-256):

```bash
# Объявим пароль системному юзеру 'postgres':
$ sudo passwd postgres
# и в самой БД:
$ sudo -u postgres psql -c "ALTER USER postgres WITH ENCRYPTED PASSWORD 'postgres_super_pass_here';"  # впишем сюда наш пароль
```

Теперь перейдя обратно к нашему приложению 'bingo', пробуем запустить уже с ключем 'run\_server':

```bash
$ ./bingo run_server
panic: failed to read config data
...
```

-, понятно что неоткуда ему прочитать сам файл конфига, и ключа соответствующего нет вида '*\-c* ' или '*\--config* ', а значит путь как-то захардкожен в сам исполняемый файл. И тут как раз нам в помощь strace:

```bash
$ strace ./bingo run_server
...
openat(AT_FDCWD, "/opt/bingo/config.yaml", O_RDONLY|O_CLOEXEC) = 6
...
```

-, из всего вывода обращаем внимание на это, вот наш путь. Создадим его вручную:

```bash
$ sudo mkdir /opt/bingo
$ sudo chown -R muhamed:muhamed /opt/bingo
```

-, юзер из под кого работаю - 'muhamed', себя собственно и делаем владельцем директории.

Создадим дефолтный конфиг по данному пути:  
`$ ./bingo print_default_config > /opt/bingo/config.yaml`  
-, и отредактируем креденшиалсы (тут моей глупостью было изначально закрыть глаза на `db_name: postgres`, думал ну мне бы добиться самого запуска, поэтому единственное что отредактировал на данном этапе, это вписал пароль от юзера/роли postgres: `password: 'пароль_от_юзера_postgres_выше_и_без_кавычек'`). Запустив:  
`$ ./bingo prepare_db`

- , далее побежала куча трейсов создания/заполнения непосредственно данными:

```
...
{"level":"warn","timestamp":"2023-11-28T05:43:26.759+0500","caller":"database/migration.go:142","msg":"Failed insertion session","session":{"start_time":"2015-10-06T11:16:31.395859924Z","customer_id":339254,"movie_id":22773},"error":"sql: statement is closed"}
{"level":"warn","timestamp":"2023-11-28T05:43:26.759+0500","caller":"database/migration.go:142","msg":"Failed insertion session","session":{"start_time":"2021-10-29T22:54:47.745623708Z","customer_id":204120,"movie_id":4780},"error":"sql: statement is closed"}
...
```

-, пока это всё работает, тут же побежал в pgAdmin4, думая что может приложение создало для себя отдельную БД, мол вот погляжу хоть что понасоздавало, какие таблички и т.д., таковой БД отдельной не обнаружив, а была единственная БД - 'postgres', перечитав вновь сам конфиг приложения и пролистав таблицы - понял что нужно было явно прописать и создать для приложения отдельную БД ;-), а еще кстати при дефолтном конфиге еще и ругалось явно так:

```
Attention!!! You are using a default username. Use your own.
Such a solution will not pass verification. It will be your own fault.
```

-, намек поняли, в общем оставив таблицу 'schema_migrations' - грохнул созданные явно таблицы.

Ну и теперь работаем "по-приличному")). Создаем нового пользователя/роль 'bingo' в БД postgres и тут же создадим отдельную БД 'bingo' владельцем коей и будет данная роль:

```
sudo -u postgres createuser bingo --interactive
sudo -u postgres psql -c "ALTER USER bingo WITH ENCRYPTED PASSWORD 'bingo_super_pass_here';" 
sudo -u postgres psql -c "CREATE DATABASE bingo WITH OWNER = bingo"
```

Далее производим непосредственные корректировки в конфиге **/opt/bingo/config.yaml**. И с более чистой совестию запускаем вновь '**prepare\_db**'. Операция длилась на моей машине наверно минут 15 и наконец-то завершилась:

```
...
{"level":"info","timestamp":"2023-11-28T13:21:43.997+0500","caller":"database/migration.go:146","msg":"Session inserted","i":4997000,"total":5000000}
{"level":"info","timestamp":"2023-11-28T13:21:44.165+0500","caller":"database/migration.go:146","msg":"Session inserted","i":4998000,"total":5000000}
{"level":"info","timestamp":"2023-11-28T13:21:44.335+0500","caller":"database/migration.go:146","msg":"Session inserted","i":4999000,"total":5000000}
{"level":"info","timestamp":"2023-11-28T13:21:44.507+0500","caller":"database/migration.go:156","msg":"Data initialisation compleat."}
muhamed@myubuntu:~/Documents/y-y-final$
```

Теперь уж запустится, думаю я, и запускаюсь с субкомандой **run_server**:

```
$ ./bingo run_server
panic: failed to build logger

goroutine 1 [running]:
bingo/internal/logger.New({0xc00003a810, 0x23}, {0xd34748, 0xd})
        /build/internal/logger/logger.go:51 +0x9e5
bingo/internal/app.New({_, _}, {{{0xc00003fa00, 0x1f}, {{0xc000010468, 0x1, 0x1}, {0xc00028fb38, 0x5}, {0xc00028fb50, ...}, ...}}, ...})
        /build/internal/app/app.go:31 +0x7a
```

-, а вот мне и на, еще один депенденси, бежим вновь трейсить с strace:

```
...
openat(AT_FDCWD, "/opt/bongo/logs/8e622be107/main.log", O_WRONLY|O_CREAT|O_APPEND|O_CLOEXEC, 0666) = -1 ENOENT (No such file or directory)
...
```

-, благо и тут снова ясно, единственое возмущение вызывало имя директории 8e622be107 - мол автогенерится ли оно или такое же фиксированное, решил прогнать несколько запусков, оказалось что не меняется (хотя в последних страницах ветки чата шло обсуждение, что это зависит от адреса email, но углубляться не стал, у меня он не меняется), т.е. фиксированное значение имя данной диры, так и создадим и присвоим права:

```
$ sudo mkdir -p /opt/bongo/logs/8e622be107
$ sudo chown -R muhamed:muhamed /opt/bongo/
```

Запускаем вновь, и после некоторого десятка другого секунд ожидания:

```
$ ./bingo run_server

My congratulations.
You were able to start the server.
Here's a secret code that confirms that you did it.
--------------------------------------------------
code:         yoohoo_server_launched
--------------------------------------------------
```

-, эврика! Идем дальше.

Пробежавшись по таблицам и прочему видно, что не имеется никаких индексов, это будет тормозить выполнение запросов, временно включим логирование запросов в БД, для этого у меня обычно следующий сниппет:

```bash
$ sudo <<EOF >> /etc/postgresql/16/main/postgresql.conf
log_destination = 'stderr'
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%a.log'
log_rotation_age = 1d
log_rotation_size = 500MB
log_truncate_on_rotation = on
log_min_duration_statement = 1000
log_checkpoints = on
#log_connections = off
#log_disconnections = off
log_duration = on
log_lock_waits = on
log_statement = 'all'
#log_temp_files = -1
log_timezone = 'Asia/Ashgabat'
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
EOF
sudo systemctl restart postgresql
```

Запустив БД и далее само приложение стал тестить каждый из перечисленных путей в инструкциях, кстати второй код **index\_page\_is\_awesome** получаем обратившись в корень, наверно либо корень, либо путь /ping были самыми первыми обращениями. В итоге наблюдая за каждым путём обращения к приложению и к соответствующему промелькнувшему в логах SQL-запросу, тут же отыгрывал его повторно в pgAdmin4 с включением анализатора запросов благо там оно умеет строить красивые графы и таблички, не буду перечислять все пункты, наверно наиболее сложным, если не изменит память, был запрос по которому в итоге построил комбинированный индекс, предпоследний из списка создания индексов ниже:

```
sudo -u postgres psql -c "CREATE INDEX indx_movies_id ON movies(id);"
sudo -u postgres psql -c "CREATE INDEX indx_customers_id ON customers(id);"
sudo -u postgres psql -c "CREATE INDEX indx_sessions_id ON sessions(id);"
sudo -u postgres psql -c "CREATE INDEX indx_comb_movies ON movies(year DESC, name ASC, id DESC) WITH (deduplicate_items=True);"
sudo -u postgres psql -c "CREATE INDEX indx_comb_customers ON customers(surname ASC, name ASC, birthday DESC, id DESC) WITH (deduplicate_items=True);"
sudo -u postgres psql -c "CREATE INDEX indx_sessions_movie_id ON sessions(movie_id);"
```

Решил также поиграться и увеличить таки буферы и кешы СУБД, а также прочитал про библиотеку '**pg\_prewarm**' - если я правильно понял принцип, то оно пытается постоянно подгружать данные в кеши ОЗУ:

```
shared_buffers = 256MB
work_mem = 64MB
maintenance_work_mem = 128MB
effective_cache_size = 512MB
enable_partitionwise_aggregate = on
enable_partitionwise_join = on
shared_preload_libraries = 'pg_prewarm'
pg_prewarm.autoprewarm = true
pg_prewarm.autoprewarm_interval = 300s
```

Очень раздосадовал факт ненахождения почему приложение так долго стартует, даже нашел было просто код с помощью эдакого читерства, я уже нашел было 2 кода: 1ый - при успешном старте, 2-ой - в ответе HTTP GET запроса на корень, т.е. curl http://localhost:7849/, в обоих случаях выдает "code: ...", соответственно можно найти другие вхождения:

```
$ strings bingo | grep "code:"
...
tats-Unis (les)crypto/rsa: input must be hashed with given hashx509: X25519 key encoded with illegal parametersx509: SAN uniformResourceIdentifier is malformedx509: IP constraint contained value of length %dx509: internal error: cannot parse constraint %qx509: internal error: URI SAN %q failed to parsex509: only RSA, ECDSA and Ed25519 keys supportedout points to big.Int, but defaultValue does notparsing/packing of this type isn't available yetInt.GobDecode: encoding version %d not supportedinvalid length for CopyOutResponse.OverallFormat# bash completion for %-36s -*- shell-script -*-
code:         yoohoo_server_launched
code:         google_dns_is_not_http
code:         index_page_is_awesome
$
```

-, вот они группкою идут видимо один массив, и всего получается их 3. Игрался трейсами ставил lurk, в нем и того менее информативно, в итоге фильтруя лог-файл, обрезая всё лишнее:

```
tail -100f /opt/bongo/logs/8e622be107/main.log  | grep -v "Quota updated" | grep -v "Started updating" | grep -v "Notified all" | grep -v "Node is alive" | grep -v "I am alive" | jq '.msg'
"Core initialization finished."
"Init routes."
"Init middlewares."
"Run server."
"Shutdown server."
"Prepare app."
"Init app."
"Create core."
"Init core."
"Run initialization request."
"Failed initialization request."
```

-, вот на "**Failed initialization request**." и обратил внимание, далее всё же решил внимательнее пробежаться strace-ом, может всё же упустил я чего. В итоге наконееец таки обнаружил следующую строку:

```
...
connect(9, {sa_family=AF_INET, sin_port=htons(80), sin_addr=inet_addr("8.8.8.8")}, 16) = -1 EINPROGRESS (Operation now in progress)
...
```

-, получается пытается установить соединение к **8.8.8.8** на **80**-ый порт, что уже сомнительно, всомнив, что в описании целевого кода мелькала связь с google, скоррелировал в итоге. В итоге заблочив явным REJECT-ом (с DROPом не пробовал, но REJECT возвращает явный отказ, который можно программно обработать) посредством iptables:  
`sudo iptables -I OUTPUT -p tcp -d 8.8.8.8 --dport 80 -j REJECT`

-, заблокировал данную связку 8.8.8.8:80 на "выходе" - в итоге получили профит:

```
$ ./bingo run_server

Congratulations.
You were able to figure out why
the application is slow to start and fix it.
Here's a secret code that confirms that you did it.
--------------------------------------------------
code:         google_dns_is_not_http
--------------------------------------------------


My congratulations.
You were able to start the server.
Here's a secret code that confirms that you did it.
--------------------------------------------------
code:         yoohoo_server_launched
--------------------------------------------------
```

-, вот он наш заветный.

Ну что ж, БД мы соптимизировали, далее для кеширования (особенно для) /long\_dummy нам потребуется старый добрый nginx, а т.к. хотелось бы его расширить и до более менее нормального мониторинга, решил остановить выбор на фреймворке Openresty (были мысли также и про допфичу http/3, но признаться, из за малого остатка времени до дедлайна решил просто опустить этот http/3 функционал), т.к. с Lua-модулями он неплохо расширяет свой в итоге функционал. Разворачивать решил сразу с контейнера, в голове уже закреплялась общая схема всего комплекса, этим чуть далее. Имидж выбрал из рекомендованного ими же на их ресурсах, это был аж целый репозиторий сборки образов Openresty https://github.com/openresty/docker-openresty, сам образ выбрал openresty/openresty:1.21.4.3-2-bullseye-fat , т.к. единственное чем он отличался от более slim-ового \*bullseye-ного образа - был добавлен менеджер пакетов opm для установки lua-модулей, а для мониторинга я выбрал модуль **knyar/nginx-lua-prometheus**, собственно так и родились первые версии Dockerfile reverseproxy (см.репозиторий), которые конечно по итогу видоизменялись.

Само приложение bingo хотел изначально запихнуть в distroless-образ как водной из домашек, где вторым из multistage билда и был этот образ урезанного debian12, запихнуть то запихнул, папку с конфигом решил подмаунтить заранее созданный путь с конфигом на хостовой ОС, а также и путь до лог-директории, были нюансы с правами к папке логов, игрался потом с debug-образом чтобы понять в чем затыки, ну благо это еще было мелочи. В целом контейнер distroless стартовал, принимал и обслуживал запросы.

Получается, в первых версиях, наш стек приложения: это стек **reverseproxy**+**bingoapp** контейнеров, запускаемых **docker-compose** скриптом, всё в добрых традициях домашки №2, на ноде COI-образца, ну и terraform обёрткой той же (потом правда немного видоизмененной), самих нод/виртуалок также 2 ед., обёрнутых в инстанс-группу и сверху покрытые лоад-балансером, который смотрит на контролируемую инстанс-группой таргет-группу. По App-стеку вроде бы отвязались.

Вот относительно БД долго думал, как бы развернуть, стоит ли пытаться разворачивать отказоустойчивость, но благо в Инструкциях и далее по чат-ветке понял что это не обязательно, хоть и приветствуется, ну и всё также лимиты времени, решил просто развернуть на отдельной виртуалке, по-соседству с будущей App-инстанс-группой. Для БД было желание выбрать диск SSD, но в облаке Yandex на объем SSD-хранилище на нас было выделено вроде 316ГБ если память не изменяет, в ошибке при создании которая ворнином оповестила, тогда решил что Ок, создадим и на обычных винтах, но вместо планируемого 1ГБ ОЗУ возьмем чуть больше)) - 2 ГБ, понадеемся на индексы и кеши, ну и на не юзаемый ранее pg\_prewarm. Решил, что наполнять стенд БД в облачной VM посредством "prepare\_db" приложения уже нет смысла, а т.к. я в самих данных ничего не менял, а только добавил индексы - снял бэкап `pg_dump bingo | gzip -9 > ./bingo_db_backup.sql.gz` , и залил/развернул уже на целевой БД-ноде.

HTTPS реализовать решил также сразу же, был свободный домен (далее - glacia.site в конфигах openresty), ну и certbot-ом пользоваться приноровились, решил тут же создать wildcard-сертификат, благо привязки к конечному IP (лоадбалансер Yandex которым должен был стать, но пока не были готовы контейнеры) при выдачи сертификата - не нужно было при DNS-challenge запросе. Ну и также добил конфиг-стек Openresty, с кешированием по нужным путям, а точнее с включением кеширования вообще на всё, а по точечным путям-location-ам вырубать кеширование. Первые прогоны wrk-ом локально на своей машине показали успех, кстати кешировать прописал максимум 50 секунд, думал ну может 59 поставить, а там вдруг временной сдвиг и я пострадаю от этого, пусть лучше по-меньше, в итоге опустил до 50 секунд. Надеялся также на следующий набор настроек:  
```  
proxy_cache_revalidate on;  
proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;  
proxy_cache_background_update on;

```
- и на быстрый старт приложения, но не тут то было. Первый же прогон Пети и треть всего - красное((.  Еще также заметил эдакое странное поведение программы при блокировке гугл-а, у меня на локальной машине при таких же настройках правил iptables - сообщение с кодом **google_dns_is_not_http** выходит, а на нодах в облаке - нет, но при этом принимать соединения бинарник начинает также сразу же, проверял, при kill-янии приложения задержка буквально на старт, вывод "...yoohoo_server_launched" выходит практически также, но при этом Петины проверки всё равно как минимум ругались на Отказоустойчивостях. И да, каким-то образом Петины тесты заваливали обе ноды приложений bingo подчистую, при любом запросе уже выдавали "I feel bad" бедненькие, смотрел по логам реверсных прокси, так и по логам запросов SQL в БД - ничего явного не заметил, но заметил только что перед отмиранием была уйма запросов эндпоинт /operation методом POST. Хотелось воспроизвести подобное на своем локальном стенде, при этом в одном из тестов на одной из облачных нод записал tcpdump-ом обращения реверс-прокси-контейнера на порт bingo (`tcpdump -i any port 7849 -w dump.pcap`), скачав дамп и пробежавшись wireshark-ом понял что всё что оно отправляло в болшинстве своем так это JSON: `{"operation": 0}`, а ответ отправляло уже с данными о хосте (видимо этим самым и опознается падение ноды и переключения на другую, а может и иная доп.инфа для анализа), запустив ab с аналогичным запросом на локальный стенд - не добился уставания приложения. В итоге было решено, что нужна еще одна доп.проверка, так то внешний LB Яндекса постоянно пингует свой бякенд инстанс-группу нод, но естественно нет механизма на перезапуск с его стороны, он просто по итогу помечал сначала одну ноду мертвой, потом вторую, и стенд замирал, и Петины проверки доизголявшись светили мне красным. Был правда удивительным факт, что в таком состоянии приложение на обоих нодах могло далее находиться продолжительное время, так то обычно оно могло раз в 10-20 минут отрубиться (эдакий искусственный баг), но docker-compose уж просто отмирание вычислял и перезапускал контейнер, а такое пред-смертное "I feel bad", но на работающем процессе - собственно никакого перезапуска. По итогу: ввел healthcheck в docker-compose.yaml:
```
healthcheck:
      test: curl -f --retry 2 --max-time 1 --retry-delay 0 --retry-max-time 2 "http://localhost:7849/ping" || kill 1
      start_period: 4s
      interval: 2s
      timeout: 5s
      retries: 2
```

-, благодаря stackoverflow/reddit-ам было собрано несколько версий "опознования" оплохения здоровья контейнера, тестами до-корректил таймауты и retry-count-ы curl-а, сначала была вариация хелсчека которая просто выдавала по итогу `|| exit 1` и после очередной проверки понял что никакого перезапуска docker-compose не проводил, и потом я вычитал, что вне Swarm-а докер не будет перезапускать unhealthy-контейнер, и то что нужно брать перезапуск на себя. Предпоследней реализацией было замудреное: `|| bash -c 'kill -s 15 -1 && (sleep 10; kill -s 9 -1)'` , где поменял только на sleep 1 из всего, но никак оно не срабатывало, и самым действенным методом оказался `|| kill 1` , т.к. просто самому основному процессу 'bingo' со своими же правами посылает по сути сигнал остановись, и права лишние не нужны и это сработало. Аа, кстати, перед этим то сам curl нужно было внедрить в сам контейнер, был то distroless, времени оставалось еще меньше, и я просто решил особо не заморачиваться и остановиться на образе **debian:bullseye-slim** , благо сам контейнер Openresty берет оттуда своё начало, и не яро тяжеловесно по итогу, но хотя бы и шелл есть и заветный curl, который только нужно было до-установить, но это вообще в этом образе не проблема. Правда там уже не оказалось группы:юзера **nonroot:nonroot** под которым стартовала прежняя версия сборки, но методом внутреннего пробега по нутру контейнера понял что для этого в bullseye-slim имеется **nobody:nogroup**, и только нужно было скорректировать еще в **cloud-config.yaml**\-е соответствующий runcmd джоб, который создавал папки для конфига и лога и делал владельцем лог-папки уже **65534:65534** (см`./terraform/step2/cloud-config.yaml`). По итогу уже только, не припомню точно, то ли один уже оставался пункт по Отказоустойчивости, или пара, но уже был успех. Получается что оставались провалы в момент падения какой-то из нод, которую верхний L4 лоадбалансер Яндекса не прям уж моментально помечал как проблемную, а т.е. трафик всё же проскальзывал и какой-то набор 50х-ток зачитывался Петиным комплексом.

Ну и последним штрихом и гвоздиком в гробик отказонеустойчивости была хитрая конфигурация Openresty, которая должна была отслеживать ответ от своих апстримов, понимать что дело туго (500чка ошибка прилетела если или нет вовсе ответу) и перенаправлять, а точнее самому втихаря переопрашивать этим же клиентским запросом другую реплику приложения. И самая последняя схема зародилась следующей: 2 ноды VM, на каждой из нод по 2 реплики приложения bingo и по 1-ому reverse-прокси, который то по сути и берет на себя основную функцию ума по отказоустойчивости и определению отказа одной из реплик, он по сути ALB на стероидах, внешний же LB L4 - просто разбавитель TCP-сессий по VM-нодам в данном случае. А чтобы было честнее к первичной инструкции, что мол вот нужно 2 ноды, многие а то нодами могут/хотят/ (о да здраствует ветка чата) назвать также и сами контейнеры, но в моем всё же понимании это больше вычислительная сущность в физических более менее рамках изолированности (уровень виртуализации VM всё же также к этому подходит в большей степени, если разнесенность VM по физическим серверам соблюдена, либо соблюдено честное разделение ресурсов на одной физической машине), а на одной ноде - может быть н-ное кол-во реплик одного и того же приложения, но повторюсь вновь - для честности я реализовал схему, когда вторая реплика становится активной только тогда, и только на тот момент, покуда отваливается основная реплика, т.е. мы как бы обманули баг приложения и в целом придерживаемся того, что у нас не будет мощи 4х реплик, а только 2х, т.к. в единый момент времени на любой из двух нод в обслуживании трафика - только одна из реплик. Чтобы это реализовать, а точнее сначала понять (и то пока походу я так и не понял до конца ;-) , но итоговая схема проверена и работает), ранее не реализовывал еще такое, пришлось перелопатить также кучу статей, там и статья на самом Nginx о захвате 5хх (https://www.nginx.com/blog/capturing-5xx-errors-debug-server/) и в переводе оной на хабре, там и stackoverflow, в конфигах openresty сначала пытался реализовать перенаправления на @debug, а точнее на @backup данным методом отлова, но мониторя живой трафик (как и дамп tcpdump-ом), включая нагрузку с wrk и фильтруя лог bingo (`tail -100f /opt/bongo/logs/8e622be107/main.log | grep -v "Quota updated" | grep -v "Started updating" | grep -v "Notified all" | grep -v "Node is alive" | grep -v "I am alive" | grep -v "am fine" | jq '.msg'`), kill-яя выборочно одну из реплик из хост-системы, замечал только незначительный всплеск запросов, даже не всплеск а всего то несколько строк трейсов на резервной реплике и по итогу wrk рапортовал о наборе 50х-ошибок, т.е. схема не работала как я бы того хотел, и оставалось уже несколько часов до полуночи субботы)). Решил дать шанс опции, на которую смотрел изначально, но которая потом перебилась этим debug-отловом: в каждый из "некешируемых" location-ов включить **proxy\_next\_upstream** и в апстримах помимо основной объявить резервную реплику и объявить ее как **backup**\-ную, закомментил места которые должны были юзаться для отлова 50x-ошибок, запустил тест и ...шикарно, в момент убивания основной реплики на резервной прям шквалом начинает бегать трафик, и после перезапуска основной трафик переключается уже на основную реплику, всё как и хотелось. Так и Петин комплекс за час до закрытия тестов наконец-таки показал мне всё зеленое.

P.S. Были также нюансы и нервные поиски за пару часов до дедлайна как же завести обе реплики на одной хост-системе, чтобы не ругались на занятость портов, т.к. **network-mode:** изначально был **host**, в итоге получилось всё засадкою реплик приложения на **network\_mode: bridge** и заменой +1 на порту реплики со стороны хоста/бриджа и всё запустилось, но сломался быстрый запуск ;-), тогда после недолгих копаний решил дополнительно объявить лок гугла и в чейне **DOCKER-USER**, и это помогло в итоге. А вообще конечно была уйма мелких нюансов, которых наверно нет смысла и времени тут перечислять, но благо большую их часть удалось решить, что-то отложить, а что-то закостылять, конечно сейчас наблюдая на реализацию понимаю что многое можно было сделать где-то немногим, а где-то кардинальнее - оптимальнее и более автоматизированно, чем сейчас. Но радует что пара последующих запуска на облачном стенде уже после дедлайна также завершились успехом. Задание вышло весьма интереснейшим, спасибо вашей команде, кто работал над этим проектом!

P.S.S. README.md начинал описывать в традициях англоязычных todo- чек листов, но уже к середине понял, что лучше было бы изначально вести RU-версию, но было уже поздно, в целом конечно всё должно быть достаточно понятно и в стилистике англоязычных деплойментов, я и не профи англ.языка.